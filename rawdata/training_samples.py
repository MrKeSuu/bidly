# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.13.1
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %%
def optimize_cell_width():
    from IPython.core.display import display, HTML
    display(HTML("<style>.container { width:100% !important; }</style>"))

optimize_cell_width()

# %% [markdown]
# # Creating a playing cards dataset
# This notebook is a guide through the creation of a dataset of playing cards. The cards are labeled with their name (ex: "2c" for "2 of spades", "Kh" for King for hearts) and with the bounding boxes delimiting their printed corners.
# > _Why bounding boxes around the corners, and not around the whole card ?_<br>Because in real conditions, more often than not, cards are partially covered. And the corner of a card is the minimum information you need to identify it.
#
# This dataset can be used for the training of a neural net intended to detect/localize playing cards. It was used on the project __[Playing card detection with YOLO v3](https://youtu.be/pnntrewH0xg)__
#
# <img src="data-nb/img/ex_generated_image.png" alt="Example of generated image "  title="Example of generated image " />
#

# %% [markdown]
# # Prerequisites 
#
# ### A. In addition to opencv and numpy, you need the following python packages:
# 1. **imgaug** : https://github.com/aleju/imgaug 
# > Helps with image augmentation
# 2. **shapely** : https://github.com/Toblerity/Shapely
# > For the manipulation and analysis of geometric objects in the Cartesian plane. It is useful here when we want to check if the bounding box of a card corner is covered by another card
# 3. **tqdm** : https://github.com/tqdm/tqdm
# > A progress bar tool. Not mandatory but convenient when you generate thousands of images
#
# ### B. Get the Describable Textures Dataset (DTD)
# > A collection of textural images in the wild (https://www.robots.ox.ac.uk/~vgg/data/dtd/). It is probably not its original goal, but it is used here as an easy way to generate various backgrounds for our images.
#
# ### C. A real deck of cards
# > This the only physical "data" you need. You have to make some measurements on the cards, and to take a picture/shoot a movie of each of the 52 cards of the deck. Everything else will be generated by scripts.
#

# %% [markdown]
# #### C.1 Measurements on the cards
# Use a ruler to measure the dimensions as indicated in the image below. 
# For the corner* measures, the idea is to delimit one rectangular zone that can hold every value+suit. The size of the marks may vary with the value or the suit, so take the measures on the cards with the widest, the tallest symbols, and add one or two millimeters. 
# <img src="data-nb/img/measures.png" alt="Measures" title="" />
# Report the measures in mm in the cell below (and don't forget to run the cell).

# %%
cardW=57
cardH=88
cornerXmin=2
cornerXmax=9.5
cornerYmin=4
cornerYmax=23.5

# We convert the measures from mm to pixels: multiply by an arbitrary factor 'zoom'
# You shouldn't need to change this
zoom=4
cardW*=zoom
cardH*=zoom
cornerXmin=int(cornerXmin*zoom)
cornerXmax=int(cornerXmax*zoom)
cornerYmin=int(cornerYmin*zoom)
cornerYmax=int(cornerYmax*zoom)

# %% [markdown]
# #### C.2 Shoot one video per card
# That's what I did in the project "Playing card detection with YOLO v3", and the scripts in this notebook were written to deal with videos. A video is interesting only if you can vary the brightness and color temperature of the light during the shot. Watch [Playing card detection with YOLO v3](https://www.youtube.com/watch?v=pnntrewH0xg&t=4m10s) to have an idea. 
# With hindsight, I am confident that you could use one picture per card instead of a video without affecting the efficiency of the neural net. The scripts could be easily adapted to deal with pictures.
# Whether you use videos or pictures, the scene must be simple, with a uniform background behind the card, like below. This way, the extraction of the card from the scene will be easy.
# <img src="data-nb/test/scene.png" alt="Scene" title="Scene" />

# %% [markdown]
# ## Imports

# %%
import numpy as np
import cv2
import os
from tqdm import tqdm
import random
import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.patches as patches
import pickle
from glob import glob 
import imgaug as ia
from imgaug import augmenters as iaa
from shapely.geometry import Polygon


# %% [markdown]
# ## Some convenient functions used in this notebook

# %%

def display_img(img,polygons=[],channels="bgr",size=9):
    """
        Function to display an inline image, and draw optional polygons (bounding boxes, convex hulls) on it.
        Use the param 'channels' to specify the order of the channels ("bgr" for an image coming from OpenCV world)
    """
    if not isinstance(polygons,list):
        polygons=[polygons]    
    if channels=="bgr": # bgr (cv2 image)
        nb_channels=img.shape[2]
        if nb_channels==4:
            img=cv2.cvtColor(img,cv2.COLOR_BGRA2RGBA)
        else:
            img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)    
    fig,ax=plt.subplots(figsize=(size,size))
    ax.set_facecolor((0,0,0))
    ax.imshow(img)
    for polygon in polygons:
        # An polygon has either shape (n,2), 
        # either (n,1,2) if it is a cv2 contour (like convex hull).
        # In the latter case, reshape in (n,2)
        if len(polygon.shape)==3:
            polygon=polygon.reshape(-1,2)
        patch=patches.Polygon(polygon,linewidth=1,edgecolor='g',facecolor='none')
        ax.add_patch(patch)

def get_filename(dirname, suffixes, prefix=""):
    """
        Function that returns a filename or a list of filenames in directory 'dirname'
        that does not exist yet. If 'suffixes' is a list, one filename per suffix in 'suffixes':
        filename = dirname + "/" + prefix + random number + "." + suffix
        Same random number for all the file name
        Ex: 
        > get_filename("dir","jpg", prefix="prefix")
        'dir/prefix408290659.jpg'
        > get_filename("dir",["jpg","xml"])
        ['dir/877739594.jpg', 'dir/877739594.xml']        
    """
    if not isinstance(suffixes, list):
        suffixes=[suffixes]
    
    suffixes=[p if p[0]=='.' else '.'+p for p in suffixes]
          
    while True:
        bname="%09d"%random.randint(0,999999999)
        fnames=[]
        for suffix in suffixes:
            fname=os.path.join(dirname,prefix+bname+suffix)
            if not os.path.isfile(fname):
                fnames.append(fname)
                
        if len(fnames) == len(suffixes): break
    
    if len(fnames)==1:
        return fnames[0]
    else:
        return fnames


# %% [markdown]
# # Define global variables

# %%
data_dir="data" # Directory that will contain all kinds of data (the data we download and the data we generate)

if not os.path.isdir(data_dir):
    os.makedirs(data_dir)

card_suits=['s','h','d','c']
card_values=['A','K','Q','J','10','9','8','7','6','5','4','3','2']

# Pickle file containing the background images from the DTD
backgrounds_pck_fn=data_dir+"/backgrounds-{}.pck"

# Pickle file containing the card images
cards_pck_fn=data_dir+"/cards-{}.pck"


# imgW,imgH: dimensions of the generated dataset images 
imgW=720
imgH=720


refCard=np.array([[0,0],[cardW,0],[cardW,cardH],[0,cardH]],dtype=np.float32)
refCardRot=np.array([[cardW,0],[cardW,cardH],[0,cardH],[0,0]],dtype=np.float32)
refCornerHL=np.array([[cornerXmin,cornerYmin],[cornerXmax,cornerYmin],[cornerXmax,cornerYmax],[cornerXmin,cornerYmax]],dtype=np.float32)
refCornerLR=np.array([[cardW-cornerXmax,cardH-cornerYmax],[cardW-cornerXmin,cardH-cornerYmax],[cardW-cornerXmin,cardH-cornerYmin],[cardW-cornerXmax,cardH-cornerYmin]],dtype=np.float32)
refCorners=np.array([refCornerHL,refCornerLR])


# %% [markdown]
# # Get Describable Textures Dataset (DTD)
# A convenient way to generate backgrounds for the images of the cards dataset

# %% [markdown]
# ### Download DTD (1x)

# %%
# # !wget https://www.robots.ox.ac.uk/~vgg/data/dtd/download/dtd-r1.0.1.tar.gz

# %% [markdown]
# ### Extract the DTD (1x)

# %%
# # !tar xf dtd-r1.0.1.tar.gz

# %% [markdown]
# ### Load all *jpg from dtd subdirectories and save them in a pickle file (1x)
#
# The next times, we will directly load the pickle file 

# %%
# dtd_dir="dtd/images/"
# bg_images=[]
# for i, subdir in enumerate(glob(dtd_dir+"/*")):
#     for f in glob(subdir+"/*.jpg"):
#         bg_images.append(mpimg.imread(f))
#     pickle.dump(bg_images,open(backgrounds_pck_fn.format(i),'wb'))
#     print("Num of images loaded :",len(bg_images))
#     print("Saved in :",backgrounds_pck_fn.format(i))
#     bg_images=[]
    
# 0 - 46.pck files

# %%
# # Clean-up
# # !rm -r dtd
# # !rm dtd-r1.0.1.tar.gz

# %% [markdown]
# ### Load the backgounds pickle file in 'backgrounds'
# 'backgrounds' is an instance of the class Backgrounds
# To get a random background image, call the method : backgrounds.get_random

# %%
class Backgrounds():
    FN_PATTERN = 'backgrounds*.pck'

    def get_random(self, display=False):
        random_filename = random.choice(glob(f'{data_dir}/{self.FN_PATTERN}'))
        random_images = pickle.load(open(random_filename, 'rb'))
        bg = random.choice(random_images)
        
        if display:
            plt.imshow(bg)
        return bg
    
    def get_total_images(self):
        total = 0
        for fn in glob(f'{data_dir}/{self.FN_PATTERN}'):
            images_this_file = pickle.load(open(fn,'rb'))
            total += len(images_this_file)
        return total


# %%
backgrounds = Backgrounds()
backgrounds.get_total_images()

# %%
# Test: display a random background
backgrounds = Backgrounds()
__ = backgrounds.get_random(display=True)

# %% [markdown]
# # Extraction of the cards from pictures or video 

# %% [markdown]
# ### Define the alphamask
# The alphamask has 2 purposes:
# - clean the border of the detected cards,
# - make that border transparent. Cards are not perfect rectangles because corners are rounded. We need to make transparent the zone between the real card and its bounding rectangle, otherwise this zone will be visible in the final generated images of the dataset
#

# %%
bord_size=2 # bord_size alpha=0
alphamask=np.ones((cardH,cardW),dtype=np.uint8)*255
cv2.rectangle(alphamask,(0,0),(cardW-1,cardH-1),0,bord_size)
cv2.line(alphamask,(bord_size*3,0),(0,bord_size*3),0,bord_size)
cv2.line(alphamask,(cardW-bord_size*3,0),(cardW,bord_size*3),0,bord_size)
cv2.line(alphamask,(0,cardH-bord_size*3),(bord_size*3,cardH),0,bord_size)
cv2.line(alphamask,(cardW-bord_size*3,cardH),(cardW,cardH-bord_size*3),0,bord_size)
plt.figure(figsize=(10,10))
plt.imshow(alphamask)


# %% [markdown]
# ## Function extract_card 
# Extract from scene image (cv2/bgr) the part corresponding to the card and transforms it 
# to fit into the reference card shape.
# We suppose here that the user facilitates as much as he can the extraction task by
# making the scene image simple (one card on uniform backgroung, not too blurry, correct lighting,...)

# %%
def varianceOfLaplacian(img):
    """
    Compute the Laplacian of the image and then return the focus
    measure, which is simply the variance of the Laplacian
    Source: A.Rosebrock, https://www.pyimagesearch.com/2015/09/07/blur-detection-with-opencv/
    """
    return cv2.Laplacian(img, cv2.CV_64F).var()

def extract_card (img, output_fn=None, min_focus=120, debug=False):
    """
    """
    
    imgwarp=None
    
    # Check the image is not too blurry
    focus=varianceOfLaplacian(img)
    if focus < min_focus: 
        if debug: print("Focus too low :", focus)
        return False,None
    
    # Convert in gray color
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # Noise-reducing and edge-preserving filter
    gray=cv2.bilateralFilter(gray,11,17,17)
    
    # Edge extraction
    edge=cv2.Canny(gray,30,200)
    
    # Find the contours in the edged image
    cnts, _ = cv2.findContours(edge.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # We suppose that the contour with largest area corresponds to the contour delimiting the card
    cnt = sorted(cnts, key = cv2.contourArea, reverse = True)[0]
    
    # We want to check that 'cnt' is the contour of a rectangular shape
    # First, determine 'box', the minimum area bounding rectangle of 'cnt'
    # Then compare area of 'cnt' and area of 'box'
    # Both areas sould be very close
    rect=cv2.minAreaRect(cnt)
    box=cv2.boxPoints(rect)
    box=np.int0(box)
    areaCnt=cv2.contourArea(cnt)
    areaBox=cv2.contourArea(box)
    valid=areaCnt/areaBox>0.95
    
    if valid:
        # We want transform the zone inside the contour into the reference rectangle of dimensions (cardW,cardH)
        ((xr,yr),(wr,hr),thetar)=rect
        # Determine 'Mp' the transformation that transforms 'box' into the reference rectangle
        if wr>hr:
            Mp=cv2.getPerspectiveTransform(np.float32(box),refCard)
        else:
            Mp=cv2.getPerspectiveTransform(np.float32(box),refCardRot)
        # Determine the warped image by applying the transformation to the image
        imgwarp=cv2.warpPerspective(img,Mp,(cardW,cardH))
        # Add alpha layer
        imgwarp=cv2.cvtColor(imgwarp,cv2.COLOR_BGR2BGRA)
        
        # Shape of 'cnt' is (n,1,2), type=int with n = number of points
        # We reshape into (1,n,2), type=float32, before feeding to perspectiveTransform
        cnta=cnt.reshape(1,-1,2).astype(np.float32)
        # Apply the transformation 'Mp' to the contour
        cntwarp=cv2.perspectiveTransform(cnta,Mp)
        cntwarp=cntwarp.astype(np.int)
        
        # We build the alpha channel so that we have transparency on the
        # external border of the card
        # First, initialize alpha channel fully transparent
        alphachannel=np.zeros(imgwarp.shape[:2],dtype=np.uint8)
        # Then fill in the contour to make opaque this zone of the card 
        cv2.drawContours(alphachannel,cntwarp,0,255,-1)
        
        # Apply the alphamask onto the alpha channel to clean it
        alphachannel=cv2.bitwise_and(alphachannel,alphamask)
        
        # Add the alphachannel to the warped image
        imgwarp[:,:,3]=alphachannel
        
        # Save the image to file
        if output_fn is not None:
            cv2.imwrite(output_fn,imgwarp)
        
    if debug:
        cv2.imshow("Gray",gray)
        cv2.imshow("Canny",edge)
        edge_bgr=cv2.cvtColor(edge,cv2.COLOR_GRAY2BGR)
        cv2.drawContours(edge_bgr,[box],0,(0,0,255),3)
        cv2.drawContours(edge_bgr,[cnt],0,(0,255,0),-1)
        cv2.imshow("Contour with biggest area",edge_bgr)
        if valid:
            cv2.imshow("Alphachannel",alphachannel)
            cv2.imshow("Extracted card",imgwarp)

    return valid,imgwarp


# %%
# Test on one image
debug=False
img=cv2.imread("data-nb/test/scene.png")
# img=cv2.imread("data/card-photos/3h.jpg")  # because of cat hair
display_img(img)
valid,card=extract_card(img,"data-nb/test/extracted_card.png", debug=debug)
if valid:
    display_img(card)
if debug:
    cv2.waitKey(0)
    cv2.destroyAllWindows()

# %%
debug=False
def extract_cards_from_photos():
    photo_dirpath = os.path.join(data_dir, 'card-photos')
    out_dirpath = os.path.join(data_dir, 'cards')
    for name in os.listdir(photo_dirpath):
        filepath = os.path.join(photo_dirpath, name)
        out_filepath = os.path.join(out_dirpath, name)
        img = cv2.imread(filepath)
        valid, card = extract_card(img,out_filepath, debug=debug)
#         if valid:
#             display_img(card)
        if debug:
            cv2.waitKey(0)
            cv2.destroyAllWindows()

# uncomment to re-extract:
# extract_cards_from_photos()


# %% [markdown]
# ## Function extract_cards_from_video

# %%
def extract_cards_from_video(video_fn, output_dir=None, keep_ratio=5, min_focus=120, debug=False):
    """
        Extract cards from media file 'video_fn' 
        If 'output_dir' is specified, the cards are saved in 'output_dir'.
        One file per card with a random file name
        Because 2 consecutives frames are probably very similar, we don't use every frame of the video, 
        but only one every 'keep_ratio' frames
        
        Returns list of extracted images
    """
    if not os.path.isfile(video_fn):
        print(f"Video file {video_fn} does not exist !!!")
        return -1,[]
    if output_dir is not None and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    cap=cv2.VideoCapture(video_fn)
    
    frame_nb=0
    imgs_list=[]
    while True:
        ret,img=cap.read()
        if not ret: break
        # Work on every 'keep_ratio' frames
        if frame_nb%keep_ratio==0:
            if output_dir is not None:
                output_fn=get_filename(output_dir,"png")
            else:
                output_fn=None
            valid,card_img = extract_card(img,output_fn,min_focus=min_focus,debug=debug)
            if debug: 
                k=cv2.waitKey(1)
                if k==27: break
            if valid:
                imgs_list.append(card_img)
        frame_nb+=1
    
    if debug:
        cap.release()
        cv2.destroyAllWindows()
    
    return imgs_list
            
        
        

# %%
# # Test card extraction from a video   
# imgs=extract_cards_from_video(
#     "data-nb/test/2c.avi",output_dir="data-nb/test/2c",debug=True)
# print("Nb images extracted:",len(imgs))

# %% [markdown]
# # Card extraction from all the videos
# We suppose we have for each card_name (ex: 2d, Kc, Ah) one video file named 'card_name.extension' (ex: 2d.avi, Kc.avi, Ah.avi) in a common directory (ex: data/video). If you use images instead of movies, the script below should work by setting the variable 'extension' below to "jpg" or "png". 
# The cards from a video, or the card from an image, will be extracted in a subdirectory named 'card_name' placed in the directory 'imgs_dir' (ex: data/cards)
#

# %%
# video_dir="data/video"
# extension="avi"
# imgs_dir="data/cards"

# for suit in card_suits:
#     for value in card_values:
        
#         card_name=value+suit
#         video_fn=os.path.join(video_dir,card_name+"."+extension)
#         output_dir=os.path.join(imgs_dir,card_name)
#         if not os.path.isdir(output_dir):
#             os.makedirs(output_dir)
#         imgs=extract_cards_from_video(video_fn,output_dir)
#         print("Extracted images for %s : %d"%(card_name,len(imgs)))

# %% [markdown]
# ### Before going on, check that everything looks good
# We randomly choose and display one of the extracted card. We also draw on the card the 2 polygons defined by refCornerHL and refCornerLR. Check that the value and suit symbols are well inside the polygons. If not, check the hand-made measures : cardW, cardH, cornerXmin, cornerXmax, cornerYmin and cornerYmax
#

# %%
# Run a few times...
imgs_dir="data/cards"
imgs_fns=glob(imgs_dir+"/*.jpg")
img_fn=random.choice(imgs_fns)
# img_fn='data/cards/As.jpg'
display_img(cv2.imread(img_fn,cv2.IMREAD_UNCHANGED),polygons=[refCornerHL,refCornerLR])

# %% [markdown]
# # Finding the convex hulls
# <img src="data-nb/img/convex_hull.jpg" alt="Convex hull" title="Convex hull" />
# This function 'find_hull' finds the convex hull in one of the corner of a card image.
# The convex hull is the minimal convex polygon that contains both the value and the suit symbols. 
#
# When I wrote this function it needed a lot of tweaking to make it work. It works well with my deck of cards, but I can't guarantee it is adapted to other decks. Search for the keyword TWEAK in the comments to find the variable and values that may need some tweak. Some of them depend on the value of global variable 'zoom'.
# It may happen that the function can't find a valid convex hull for a card. It is not a problem, it only means that this card won't be used to generate the dataset.

# %% [markdown]
# #### Find corner zones dynamically ~~for K/Q/Js~~
# - ~~maybe try 'contour with the largest area' again, for the extracted cards~~, use simpler method using brightness
#
# #### Fix 10s (1 exluded sometimes)
# fixed, by adjusting params relating to 'center of mass'

# %%
# Checking column min brightness to determine const: MIN_BRIGHTNESS
filename = None
# filename = 'data/cards/Js.jpg'
img = read_card_img(filename)

plt_imshow('img', img)
grey = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
plt.plot(grey.min(axis=0))

# %%
MIN_BRIGHTNESS = 65

IMGS_DIR = 'data/cards'
IMGS_FILEPATHS = glob(IMGS_DIR+'/*.jpg')

def read_card_img(img_filename=None):
    if img_filename is None:
        img_filename = random.choice(IMGS_FILEPATHS)
        print(img_filename)
    img = cv2.imread(img_filename, cv2.IMREAD_UNCHANGED)
    return img

def plt_imshow(title, img):
    print(title)
    plt.imshow(img)
    plt.show()

def adjust_x(grey, x_inner, bottom_right):
    """Adjust inner x to not touch frame/symbols."""
    x = x_inner
    while True:
        if grey[:, x].min() > MIN_BRIGHTNESS:
            return x
        else:
            x = x+1 if bottom_right else x-1

def adjust_corner_zone(img, corner, bottom_right):
    x1=int(corner[0][0])
    y1=int(corner[0][1])
    x2=int(corner[2][0])
    y2=int(corner[2][1])
    
    grey =cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    x_inner = x1 if bottom_right else x2
    x_adjusted = adjust_x(grey, x_inner, bottom_right)
    if bottom_right:
        x1 = x_adjusted
        corner[0][0] = x1
    else:
        x2 = x_adjusted
        corner[2][0] = x2
    
    zone = img[y1:y2,x1:x2].copy()
    return zone, corner
    


# %%
def findHull(img, corner=refCornerHL, debug="no", bottom_right=False):
    """
        Find in the zone 'corner' of image 'img' and return, the convex hull delimiting
        the value and suit symbols
        'corner' (shape (4,2)) is an array of 4 points delimiting a rectangular zone, 
        takes one of the 2 possible values : refCornerHL or refCornerLR
        debug=
    """
    
    kernel = np.ones((3,3),np.uint8)
    corner=corner.astype(np.int)

    # We will focus on the zone of 'img' delimited by 'corner'
    w=int(corner[2][0])-int(corner[0][0])
    h=int(corner[2][1])-int(corner[0][1])
    zone, corner = adjust_corner_zone(img, corner, bottom_right)

    strange_cnt=np.zeros_like(zone)
    gray=cv2.cvtColor(zone,cv2.COLOR_BGR2GRAY)
#     plt_imshow('grey', gray)  # dev
    thld=cv2.Canny(gray,30,200)
#     plt_imshow('canny', thld)  # dev
    thld = cv2.dilate(thld,kernel,iterations=1)
    if debug!="no":
        plt_imshow('thld:', thld)
    
    # Find the contours
    contours,_=cv2.findContours(thld.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)

    min_area=30 # We will reject contours with small area. TWEAK, 'zoom' dependant
    min_solidity=0.3 # Reject contours with a low solidity. TWEAK
    
    concat_contour=None # We will aggregate in 'concat_contour' the contours that we want to keep
    
    ok=True
    for c in contours:
        area=cv2.contourArea(c)

        hull = cv2.convexHull(c)
        hull_area = cv2.contourArea(hull)
        solidity = float(area)/hull_area
        # Determine the center of gravity (cx,cy) of the contour
        M=cv2.moments(c)
        cx=int(M['m10']/M['m00'])
        cy=int(M['m01']/M['m00'])
        #  abs(w/2-cx)<w*0.3 and abs(h/2-cy)<h*0.4 : TWEAK
        # the idea here is to keep only the contours which are closed to the center of the zone
        if area >= min_area and abs(w/2-cx)<w*0.4 and abs(h/2-cy)<h*0.45 and solidity>min_solidity:
            if debug != "no" :
                cv2.drawContours(zone,[c],0,(255,0,0),-1)
            if concat_contour is None:
                concat_contour=c
            else:
                concat_contour=np.concatenate((concat_contour,c))
        if debug != "no" and solidity <= min_solidity :
            print("Solidity",solidity)
            cv2.drawContours(strange_cnt,[c],0,255,2)
            plt_imshow("Strange contours:", strange_cnt)
            
     
    if concat_contour is not None:
        # At this point, we suppose that 'concat_contour' contains 
        # only the contours corresponding the value and suit symbols   
        # We can now determine the hull
        hull=cv2.convexHull(concat_contour)
        hull_area=cv2.contourArea(hull)
        # If the area of the hull is to small or too big, there may be a problem
        min_hull_area=940 # TWEAK, deck and 'zoom' dependant
        max_hull_area=1650 # TWEAK, deck and 'zoom' dependant
        if hull_area < min_hull_area or hull_area > max_hull_area: 
            ok=False
            if debug!="no":
                print("Hull area=",hull_area,"too large or too small")
        # So far, the coordinates of the hull are relative to 'zone'
        # We need the coordinates relative to the image -> 'hull_in_img' 
        hull_in_img=hull+corner[0]

    else:
        ok=False
    
    
    if debug != "no" :
        if concat_contour is not None:
            cv2.drawContours(zone,[hull],0,(0,255,0),1)
            cv2.drawContours(img,[hull_in_img],0,(0,255,0),1)
        plt_imshow("Zone:", zone)
        plt_imshow("Image:", img)
        if ok and debug!="pause_always":
            key=cv2.waitKey(1)
        else:
            key=cv2.waitKey(0)
        if key==27:
            return None
    if ok == False:
        
        return None
    
    return hull_in_img

# %%
# Test find_hull on a random card image
# debug = "no" or "pause_always" or "pause_on_pb"
# If debug!="no", you may have to press a key to continue execution after pause
debug="no"
imgs_dir="data/cards"
imgs_fns=glob(imgs_dir+"/*.jpg")
img_fn=random.choice(imgs_fns)
# debug, img_fn = True, 'data/cards/3h.jpg'
# debug, img_fn = True, random.choice(imgs_fns)
print(img_fn)
img=cv2.imread(img_fn,cv2.IMREAD_UNCHANGED)

hullHL=findHull(img,refCornerHL,debug=debug)
hullLR=findHull(img,refCornerLR,debug=debug, bottom_right=True)
display_img(img,[refCornerHL,refCornerLR,hullHL,hullLR])

if debug!="no": cv2.destroyAllWindows()


# %% [markdown]
# ### Generate images with different lighting condition for each card
#

# %%
def adjust_brightness(img, value):
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h, s, v = cv2.split(hsv)

    if value > 0:
        lim = 255 - value
        v[v > lim] = 255
        v[v <= lim] += value
    else:
        lim = 0 - value
        v[v < lim] = 0
        v[v >= lim] -= -value

    final_hsv = cv2.merge((h, s, v))
    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)
    return img

def adjust_warmth(img, value):
    b, g, r = cv2.split(img)

    if value > 0:  # warmer
        r_lim = 255 - value
        r[r > r_lim] = 255
        r[r <= r_lim] += value
        b_lim = 0 + value
        b[b < b_lim] = 0
        b[b >= b_lim] -= value
    else:  # cooler
        r_lim = 0 - value
        r[r < r_lim] = 0
        r[r >= r_lim] -= -value
        b_lim = 255 + value
        b[b > b_lim] = 255
        b[b <= b_lim] += -value

    img = cv2.merge((b, g, r))
    return img


# %%
BRIGHTNESS_RANGE = list(range(-50, 50))
WARMTH_RANGE = list(range(-5, 25))

def random_lighting(img, verbose=False):

    delta_br = random.choice(BRIGHTNESS_RANGE)
    img_m = adjust_brightness(img, delta_br)

    delta_wm = random.choice(WARMTH_RANGE)
    img_m = adjust_warmth(img_m, delta_wm)

    if verbose:
        display_img(img, size=5)
        print('brightness:', delta_br)
        print('warmth:', delta_wm)
        display_img(img_m, size=5)
    return img_m

    
img = read_card_img()
img_m = random_lighting(img, verbose=True)
# cv2.imwrite('data/cards/adjusted.jpg', img_m)  # dev

# %%
N_IMGS_PER_CARD = 100

def apply_random_lighting():
    for filepath in sorted(IMGS_FILEPATHS):
        card_name, __ = os.path.splitext(os.path.basename(filepath))

        card_dirpath = os.path.join(IMGS_DIR, card_name)
        if not os.path.exists(card_dirpath):
            os.mkdir(card_dirpath)

        print(f"Creating {N_IMGS_PER_CARD} randomly lighted {card_name}")
        card_img = read_card_img(filepath)
        for i in range(N_IMGS_PER_CARD):
            new_name = f'{i:03}.jpg'
            new_path = os.path.join(card_dirpath, new_name)

            card_img_m = random_lighting(card_img)
            cv2.imwrite(new_path, card_img_m)

# apply_random_lighting()  # uncomment to re-apply
        


# %% [markdown]
# *YL: the following won't work as the modified `findHull` uses a const: MIN_BRIGHTNESS; instead, use orig imgs to assign static hull value to each card*
#
# ### Load all card image, calculate their convex hulls and save the whole in a pickle file (1x)
#
# The next times, we will directly load the pickle file 
# The structure saved in the pickle file is a dictionnary named 'cards' of lists of triplets (img,hullHL,hullLR). The keys of the dictionnary are the card names ("Ad","10h",... so 52 entries in the dictionnary). 

# %%
def get_orig_filepath(modified_dirpath):
    """Get the filepath the orig img before random lighting."""
    orig_filepath = modified_dirpath+'.jpg'
    return orig_filepath

# get_orig_filepath('data/cards/Ac')


# %%
imgs_dir="data/cards"
def enhance_with_hulls():
    for suit in card_suits:
        for value in card_values:
            card_name=value+suit
            card_dir=os.path.join(imgs_dir,card_name)
            if not os.path.isdir(card_dir):
                print(f"!!! {card_dir} does not exist !!!")
                continue

            # Find hull using orig imgs
            orig_img = cv2.imread(get_orig_filepath(card_dir), cv2.IMREAD_UNCHANGED)
            hullHL=findHull(orig_img,refCornerHL,debug="no")
            hullLR=findHull(orig_img,refCornerLR,debug="no",bottom_right=True)
            if hullHL is None or hullLR is None:
                print(f"!!! Card {card_dir} not used !!!")
                continue

            cards = []
            for f in glob(card_dir+"/*.jpg"):
                img=cv2.imread(f,cv2.IMREAD_UNCHANGED)
                # We store the image in "rgb" format (we don't need opencv anymore)
                img=cv2.cvtColor(img,cv2.COLOR_BGRA2RGBA)
                cards.append((img,hullHL,hullLR))
            print(f"Nb images for {card_name} : {len(cards)}")

            print("Saved in :",cards_pck_fn.format(card_name))
            pickle.dump(cards,open(cards_pck_fn.format(card_name),'wb'))

    cv2.destroyAllWindows()

# enhance_with_hulls()  # uncomment to redo


# %% [markdown]
# ### Load the cards pickle file in 'cards'
# 'cards' is an instance of the class Cards
# To get a random background image, call the method : cards.get_random() or cards.get_random(card_name) if you want a random card of a given value. Ex: cards.get_random('Ah')

# %%
class Cards():
    CARDS_PCK_FN_PATTERN = 'cards*.pck'
    CARD_NAMES = [v+s for s in card_suits for v in card_values]
        
    def get_random(self, card_name=None, display=False):
        if card_name is None:
            card_name = random.choice(self.CARD_NAMES)

        pck_filepath = cards_pck_fn.format(card_name)
        random_info = random.choice(pickle.load(open(pck_filepath, 'rb')))

        card,hull1,hull2=random_info
        if display:
            if display: display_img(card,[hull1,hull2],"rgb")
        return card,card_name,hull1,hull2
    
cards = Cards()

# %%
# Test: display a random card
_=cards.get_random(display=True)
# Display a random Ace of spades
#_=cards.get_random("As",display=True)

# %% [markdown]
# # Generating a scene
# We can now generate a scene (= image of the dataset). We are considering here only 2 kinds of scene (but nothing prevents you to add more scenarios):
# 1. a scene with 2 cards: each card is randomly transformed (scaled, rotated, translated) independantly from the other;
# 2. a scene with 3 cards : the 3 cards are grouped together (a bit randomly) to form a fan, then the group is randomly transformed.
#
# |Scene1|Scene2|
# |----|----|
# |<img src="data-nb/img/gen_2_cards.jpg" alt="Generated image with 2 cards" title="Generated image with 2 cards"/>|<img src="data-nb/img/gen_3_cards.jpg" alt="Generated image with 3 cards" title="Generated image with 3 cards"/>|

# %% [markdown]
# ### To save bounding boxes annotations in Pascal VOC format 
# http://host.robots.ox.ac.uk/pascal/VOC/voc2008/htmldoc/

# %%
xml_body_1="""<annotation>
        <folder>FOLDER</folder>
        <filename>{FILENAME}</filename>
        <path>{PATH}</path>
        <source>
                <database>Unknown</database>
        </source>
        <size>
                <width>{WIDTH}</width>
                <height>{HEIGHT}</height>
                <depth>3</depth>
        </size>
"""
xml_object=""" <object>
                <name>{CLASS}</name>
                <pose>Unspecified</pose>
                <truncated>0</truncated>
                <difficult>0</difficult>
                <bndbox>
                        <xmin>{XMIN}</xmin>
                        <ymin>{YMIN}</ymin>
                        <xmax>{XMAX}</xmax>
                        <ymax>{YMAX}</ymax>
                </bndbox>
        </object>
"""
xml_body_2="""</annotation>        
"""

def create_voc_xml(xml_file, img_file,listbba,display=False):
    with open(xml_file,"w") as f:
        f.write(xml_body_1.format(**{'FILENAME':os.path.basename(img_file), 'PATH':img_file,'WIDTH':imgW,'HEIGHT':imgH}))
        for bba in listbba:            
            f.write(xml_object.format(**{'CLASS':bba.classname,'XMIN':bba.x1,'YMIN':bba.y1,'XMAX':bba.x2,'YMAX':bba.y2}))
        f.write(xml_body_2)
        if display: print("New xml",xml_file)
        


# %%
# Scenario with 2 cards:
# The original image of a card has the shape (cardH,cardW,4)
# We first paste it in a zero image of shape (imgH,imgW,4) at position decalX, decalY
# so that the original image is centerd in the zero image
decalX = int((imgW-cardW) / 2)
decalY = int((imgH-cardH) / 2)

# Scenario with 3 cards : decal values are different
decalX3 = int(imgW/2)
decalY3 = int(imgH/2 - cardH)


def kps_to_polygon(kps):
    """
        Convert imgaug keypoints to shapely polygon
    """
    pts=[(kp.x, kp.y) for kp in kps]
    return Polygon(pts)


def hull_to_kps(hull, decalX=decalX, decalY=decalY):
    """
        Convert hull to imgaug keypoints
    """
    # hull is a cv2.Contour, shape : Nx1x2
    kps = [
        ia.Keypoint(x=p[0]+decalX,
                    y=p[1]+decalY)
        for p in hull.reshape(-1, 2)
    ]
    kps = ia.KeypointsOnImage(kps, shape=(imgH,imgW,3))
    return kps


def kps_to_BB(kps):
    """
        Determine imgaug bounding box from imgaug keypoints
    """
    extend = 3 # To make the bounding box a little bit bigger
    kpsx = [kp.x for kp in kps.keypoints]
    minx = max(0, int(min(kpsx)-extend))
    maxx = min(imgW, int(max(kpsx)+extend))

    kpsy = [kp.y for kp in kps.keypoints]
    miny = max(0, int(min(kpsy)-extend))
    maxy = min(imgH, int(max(kpsy)+extend))

    if minx == maxx or miny == maxy:
        return None
    else:
        return ia.BoundingBox(x1=minx, y1=miny, x2=maxx, y2=maxy)


# imgaug keypoints of the bounding box of a whole card
cardKP = ia.KeypointsOnImage([
    ia.Keypoint(x=decalX, y=decalY),
    ia.Keypoint(x=decalX+cardW, y=decalY),
    ia.Keypoint(x=decalX+cardW, y=decalY+cardH),
    ia.Keypoint(x=decalX, y=decalY+cardH)
], shape=(imgH,imgW,3))

# YL: ideally scale should be around 0.4 but that maybe too small (orig resolution too low)

# imgaug transformation for one card in scenario with 2 cards
transform_1card = iaa.Sequential([
    iaa.Affine(scale=[0.55, 0.9]),
    iaa.Affine(rotate=(-180, 180)),
    iaa.Affine(translate_percent={"x": (-0.25,0.25), "y": (-0.25,0.25)}),
    iaa.PerspectiveTransform(),
])

# For the 3 cards scenario, we use 3 imgaug transforms, the first 2 are for individual cards, 
# and the third one for the group of 3 cards
trans_rot1 = iaa.Sequential([
    iaa.Affine(translate_px={"x": (10, 20)}),
    iaa.Affine(rotate=(22, 30))
])
trans_rot2 = iaa.Sequential([
    iaa.Affine(translate_px={"x": (0, 5)}),
    iaa.Affine(rotate=(10, 15))
])
transform_3cards = iaa.Sequential([
    iaa.Affine(translate_px={"x": decalX-decalX3, "y": decalY-decalY3}),
    iaa.Affine(scale=[0.55, 0.9]),
    iaa.Affine(rotate=(-180, 180)),
    iaa.Affine(translate_percent={"x": (-0.2,0.2),"y": (-0.2,0.2)}),
    iaa.PerspectiveTransform(),
])

# imgaug transformation for the background
scaleBg = iaa.Resize({"height": imgH, "width": imgW})


def augment(img, list_kps, seq, restart=True):
    """
        Apply augmentation 'seq' to image 'img' and keypoints 'list_kps'.
        If restart is False, the augmentation has been made deterministic
        outside the function (used for 3 cards scenario)
    """ 
    # Make sequence deterministic
    while True:
        if restart:
            myseq = seq.to_deterministic()
        else:
            myseq = seq

        # Augment image, keypoints and bbs 
        img_aug = myseq.augment_images([img])[0]
        list_kps_aug = [
            myseq.augment_keypoints([kp])[0]
            for kp in list_kps]
        list_bbs = [
            kps_to_BB(list_kps_aug[1]),
            kps_to_BB(list_kps_aug[2])]

        # Check the card bounding box stays inside the image
        valid = True
        for bb in list_bbs:
            no_bb = bb is None
            too_right = int(round(bb.x2)) >= imgW
            too_left = int(bb.x1) <= 0
            too_low = int(round(bb.y2)) >= imgH
            too_high = int(bb.y1) <= 0
            if no_bb or too_right or too_left or too_low or too_high:
                valid = False
                break

        if valid:
            break
        elif not restart:
            img_aug = None
            break
                
    return img_aug, list_kps_aug, list_bbs


class BBA:  # Bounding box + annotations
    def __init__(self, bb, classname):
        self.x1 = int(round(bb.x1))
        self.y1 = int(round(bb.y1))
        self.x2 = int(round(bb.x2))
        self.y2 = int(round(bb.y2))
        self.classname = classname


class Scene:
    def __init__(self, bg,
                 img1, class1, hulla1, hullb1,
                 img2, class2, hulla2, hullb2,
                 img3=None, class3=None, hulla3=None, hullb3=None):
        self.class1 = class1
        self.class2 = class2
        self.class3 = class3
        
        if img3 is not None:
            self.create3CardsScene(
                bg,
                img1, class1, hulla1, hullb1,
                img2, class2, hulla2, hullb2,
                img3, class3, hulla3, hullb3)
        else:
            self.create2CardsScene(
                bg,
                img1, class1, hulla1, hullb1,
                img2, class2, hulla2, hullb2)

    def create2CardsScene(
            self, bg,
            img1, class1, hulla1, hullb1,
            img2, class2, hulla2, hullb2):

        kpsa1 = hull_to_kps(hulla1)
        kpsb1 = hull_to_kps(hullb1)
        kpsa2 = hull_to_kps(hulla2)
        kpsb2 = hull_to_kps(hullb2)
        
        # Randomly transform 1st card
        self.img1 = np.zeros((imgH,imgW,4), dtype=np.uint8)
        self.img1[decalY:decalY+cardH, decalX:decalX+cardW, :] = img1
        _augmented = augment(self.img1, [cardKP,kpsa1,kpsb1], transform_1card)
        self.img1, self.lkps1, self.bbs1 = _augmented

        # Randomly transform 2nd card.
        # We don't want that card 2 partially(10-90%) covers any hull corner of card 1.
        # If so, we apply a new random transform to card 2
        while True:
            self.listbba = []
            self.img2 = np.zeros((imgH,imgW,4), dtype=np.uint8)
            self.img2[decalY:decalY+cardH, decalX:decalX+cardW, :] = img2
            _augmented = augment(self.img2, [cardKP,kpsa2,kpsb2], transform_1card)
            self.img2, self.lkps2, self.bbs2 = _augmented

            # mainPoly2: shapely polygon of card 2
            mainPoly2 = kps_to_polygon(self.lkps2[0].keypoints[0:4])
            invalid = False
            intersect_ratio = 0.1
            for i in range(1, 3):
                # smallPoly1: shapely polygon of one of the hull of card 1
                smallPoly1 = kps_to_polygon(self.lkps1[i].keypoints[:])
                a = smallPoly1.area
                # We calculate area of the intersection of card 1 corner with card 2
                intersect = mainPoly2.intersection(smallPoly1)
                ai = intersect.area
                # If intersection area is small enough, we accept card 2
                if ai/a < intersect_ratio:  # YL: curr 0.1
                    self.listbba.append(BBA(self.bbs1[i-1], class1))
                # If intersection area is not small, but also not big enough, we want apply new transform to card 2
                elif ai/a < 1 - intersect_ratio:  # YL: curr 0.9
                    invalid = True
                    break

            if not invalid:
                break

        for bb in self.bbs2:
            self.listbba.append(BBA(bb, class2))

        # Construct final image of the scene by superimposing: bg, img1 and img2
        self.bg = scaleBg.augment_image(bg)
        mask1 = self.img1[:, :, 3]
        self.mask1 = np.stack([mask1]*3, -1)
        self.final = np.where(self.mask1, self.img1[:, :, 0:3], self.bg)
        mask2 = self.img2[:, :, 3]
        self.mask2 = np.stack([mask2]*3, -1)
        self.final = np.where(self.mask2, self.img2[:, :, 0:3], self.final)

    def create3CardsScene(
            self, bg,
            img1, class1, hulla1, hullb1,
            img2, class2, hulla2, hullb2,
            img3, class3, hulla3, hullb3):

        kpsa1 = hull_to_kps(hulla1, decalX3, decalY3)
        kpsb1 = hull_to_kps(hullb1, decalX3, decalY3)
        kpsa2 = hull_to_kps(hulla2, decalX3, decalY3)
        kpsb2 = hull_to_kps(hullb2, decalX3, decalY3)
        kpsa3 = hull_to_kps(hulla3, decalX3, decalY3)
        kpsb3 = hull_to_kps(hullb3, decalX3, decalY3)

        self.img3 = np.zeros((imgH,imgW,4), dtype=np.uint8)
        self.img3[decalY3:decalY3+cardH, decalX3:decalX3+cardW, :] = img3
        _augmented = augment(self.img3, [cardKP,kpsa3,kpsb3], trans_rot1)
        self.img3, self.lkps3, self.bbs3= _augmented

        self.img2 = np.zeros((imgH,imgW,4), dtype=np.uint8)
        self.img2[decalY3:decalY3+cardH, decalX3:decalX3+cardW, :] = img2
        _augmented = augment(self.img2, [cardKP,kpsa2,kpsb2], trans_rot2)
        self.img2, self.lkps2, self.bbs2= _augmented

        self.img1 = np.zeros((imgH,imgW,4), dtype=np.uint8)
        self.img1[decalY3:decalY3+cardH, decalX3:decalX3+cardW, :] = img1

        while True:
            det_transform_3cards = transform_3cards.to_deterministic()
            _img3, _lkps3, self.bbs3 = augment(self.img3, self.lkps3, det_transform_3cards, False)
            if _img3 is None:
                continue

            _img2, _lkps2, self.bbs2 = augment(self.img2, self.lkps2, det_transform_3cards, False)
            if _img2 is None:
                continue

            _img1, self.lkps1, self.bbs1 = augment(self.img1, [cardKP,kpsa1,kpsb1], det_transform_3cards, False)
            if _img1 is None:
                continue

            break

        self.img3 =_img3
        self.lkps3 =_lkps3
        self.img2 =_img2
        self.lkps2 =_lkps2
        self.img1 = _img1

        self.listbba = [
            BBA(self.bbs1[0], class1),
            BBA(self.bbs2[0], class2),
            BBA(self.bbs3[0], class3),
            BBA(self.bbs3[1], class3)]
        
        # Construct final image of the scene by superimposing: bg, img1, img2 and img3
        self.bg = scaleBg.augment_image(bg)
        mask1 = self.img1[:, :, 3]
        self.mask1 = np.stack([mask1]*3, -1)
        self.final = np.where(self.mask1, self.img1[:, :, 0:3], self.bg)
        mask2 = self.img2[:, :, 3]
        self.mask2 = np.stack([mask2]*3, -1)
        self.final = np.where(self.mask2, self.img2[:, :, 0:3], self.final)
        mask3 = self.img3[:, :, 3]
        self.mask3 = np.stack([mask3]*3, -1)
        self.final = np.where(self.mask3, self.img3[:, :, 0:3], self.final)

    def display(self):
        fig, ax = plt.subplots(1, figsize=(8,8))
        ax.imshow(self.final)
        for bb in self.listbba:
            rect = patches.Rectangle(
                (bb.x1, bb.y1),
                bb.x2 - bb.x1,
                bb.y2 - bb.y1,
                linewidth=1, edgecolor='b', facecolor='none')
            ax.add_patch(rect)

        print(f"With cards: {self.class1}, {self.class2}, {self.class3}")

    def res(self):
        return self.final

    def write_files(self, save_dir, display=False):
        jpg_fn, xml_fn = get_filename(save_dir, ["jpg", "xml"])

        plt.imsave(jpg_fn, self.final)
        if display:
            print("New image saved in", jpg_fn)

        create_voc_xml(xml_fn, jpg_fn, self.listbba, display=display)



# %%
# Test generation of a scene with 2 cards
bg=backgrounds.get_random()
img1,card_val1,hulla1,hullb1=cards.get_random()
img2,card_val2,hulla2,hullb2=cards.get_random()

newimg=Scene(bg,img1,card_val1,hulla1,hullb1,img2,card_val2,hulla2,hullb2)
newimg.display()

# %%
# Test generation of a scene with 3 cards
bg=backgrounds.get_random()
img1,card_val1,hulla1,hullb1=cards.get_random()
img2,card_val2,hulla2,hullb2=cards.get_random()
img3,card_val3,hulla3,hullb3=cards.get_random()

newimg=Scene(bg,img1,card_val1,hulla1,hullb1,img2,card_val2,hulla2,hullb2,img3,card_val3,hulla3,hullb3)
newimg.display()

# %% [markdown]
# ## Generate the datasets
# Typically, you want to generate a training dataset and a validation dataset of different size and different destination directory.
# Modify the variable 'nb_cards_to_generate' and 'save_dir' accordingly
#

# %%
train_dir = "data/train"
test_dir = "data/test"

N_SCENES_PER_CARD = 100
TEST_SIZE = 0.25

os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

def get_n_scenes_per_card(card_name):
    if card_name.startswith('A') or card_name.startswith('4'):
        # similar so gen a bit more:
        return N_SCENES_PER_CARD + 20
    return N_SCENES_PER_CARD


# %% [markdown]
# ### Generation of the 2 cards scenes

# %%
def gen_2_cards_scene(output_dir, main_card=None):
    bg = backgrounds.get_random()
    img1, card_val1, hulla1, hullb1 = cards.get_random()
    img2, card_val2, hulla2, hullb2 = cards.get_random(card_name=main_card)
    
    newimg = Scene(
        bg,
        img1, card_val1, hulla1, hullb1,
        img2, card_val2, hulla2, hullb2)
    newimg.write_files(output_dir)


# %%
## all random
nb_cards_to_generate = 100

for i in tqdm(range(nb_cards_to_generate)):
    gen_2_cards_scene(train_dir)


# %%
## control # images for first card
for card_name in Cards.CARD_NAMES:
    print('Generating scenes for', card_name)

    n_scenes = get_n_scenes_per_card(card_name)
    n_scenes_te = int(n_scenes * TEST_SIZE)
    n_scenes_tr = n_scenes - n_scenes_te

    for __ in tqdm(range(n_scenes_tr)):
        gen_2_cards_scene(train_dir, main_card=card_name)

    for __ in tqdm(range(n_scenes_te)):
        gen_2_cards_scene(test_dir, main_card=card_name)


# %% [markdown]
# ### Generation of the 3 cards scenes

# %%
def gen_3_cards_scene(output_dir, main_card=None):
    bg = backgrounds.get_random()
    img1, card_val1, hulla1, hullb1 = cards.get_random()
    img2, card_val2, hulla2, hullb2 = cards.get_random()
    img3, card_val3, hulla3, hullb3 = cards.get_random(card_name=main_card)
    
    newimg = Scene(
        bg,
        img1, card_val1, hulla1, hullb1,
        img2, card_val2, hulla2, hullb2,
        img3, card_val3, hulla3, hullb3)
    newimg.write_files(output_dir)


# %%
## all random
nb_cards_to_generate = 100

for i in tqdm(range(nb_cards_to_generate)):
    gen_3_cards_scene(train_dir)


# %%
## control # images for first card
for card_name in Cards.CARD_NAMES:
    print('Generating scenes for', card_name)

    n_scenes = get_n_scenes_per_card(card_name)
    n_scenes_te = int(n_scenes * TEST_SIZE)
    n_scenes_tr = n_scenes - n_scenes_te

    for __ in tqdm(range(n_scenes_tr)):
        gen_3_cards_scene(train_dir, main_card=card_name)

    for __ in tqdm(range(n_scenes_te)):
        gen_3_cards_scene(test_dir, main_card=card_name)

# %% [markdown]
# ## In case you want to train YOLO with the generated datasets
# YOLO cannot directly exploit the Pascal VOC annotations files. You need to convert the xml files in txt files accordingly to the syntax explained here: https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects
# The script 'convert_voc_yolo.py' makes this conversion and also generates the txt file that contains all the images of the dataset

# %%
# !python convert_voc_yolo.py data/scenes/train data/cards.names data/yolo_train_list.txt
# !python convert_voc_yolo.py data/scenes/test data/cards.names data/yolo_test_list.txt
